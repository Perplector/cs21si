{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 9 Class Exercises\n",
    "\n",
    "This week, we'll be learning how to apply deep learning to natural language processing (NLP), language models, recurrent neural networks. We'll motivate our discussion by trying to better understand and make predictions on text from fake news articles! Run the below cell to get started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import utils\n",
    "import io\n",
    "import random\n",
    "random.seed(100)\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import LSTM\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Neural NLP Warmup\n",
    "\n",
    "In this part, you won't code up any models, but you'll get a better understanding of the strengths and limitations of our first attempt at modeling text data using neural networks.\n",
    "\n",
    "First, finish the following function, which computes the number of input features into our model given an *embedding_size* (the size of our word vectors) and *sequence_length* (the number of words we put into our model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_input_neurons(embedding_size, sequence_length):\n",
    "    ### YOUR CODE HERE\n",
    "    return embedding_size * sequence_length\n",
    "    ### END CODE\n",
    "\n",
    "print(\"Input neurons with 300-dimensional embeddings and a sequence length of 8:\", num_input_neurons(300, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output**\n",
    "\n",
    "Input neurons with 300-dimensional embeddings and a sequence length of 8: 2400\n",
    "\n",
    "Assuming our next layer has *hidden1_size* neurons, how many parameters do we need (including both weights and bias) for the first feedforward layer? Finish the below function. You might find your previous function helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_parameters_first_layer(embedding_size, sequence_length, hidden1_size):\n",
    "    ### YOUR CODE HERE\n",
    "    return num_input_neurons(embedding_size, sequence_length) * hidden1_size + hidden1_size\n",
    "    ### END CODE\n",
    "    \n",
    "print(\"Number of params for the first layer with 100-dimensional embeddings, sequence length of 5, hidden size of 50:\", num_parameters_first_layer(100, 5, 50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output**\n",
    "\n",
    "Number of params for the first layer with 100-dimensional embeddings, sequence length of 5, hidden size of 50: 25050\n",
    "\n",
    "Now, assume we have a 3-layer layer network (input layer, hidden layer 1, hidden layer 2, output layer) that does multi-class classification with *vocabulary_size* possible classes. Using our previous function, the *embedding_size*, *sequence_length*, *hidden1_size*, *hidden2_size*, and *vocabulary_size*, calculate the number of parameters needed for the weights and biases for this network. You may find your previous function helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_parameters_feedforward(embedding_size, sequence_length, hidden1_size, hidden2_size, vocabulary_size):\n",
    "    ### YOUR CODE HERE\n",
    "    first = num_parameters_first_layer(embedding_size, sequence_length, hidden1_size)\n",
    "    second = hidden1_size * hidden2_size + hidden2_size\n",
    "    third = hidden2_size * vocabulary_size + vocabulary_size\n",
    "    return first + second + third\n",
    "    ### END CODE\n",
    "    \n",
    "print(\"Number of total params with 100-dimensional embeddings, sequence length of 5, hidden 1 size of 50, hidden 1 size of 25, vocabulary size of 100:\", num_parameters_feedforward(100, 5, 50, 25, 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output**\n",
    "\n",
    "Number of total params with 100-dimensional embeddings, sequence length of 5, hidden 1 size of 50, hidden 1 size of 25, vocabulary size of 100: 28925\n",
    "    \n",
    "This seems like a lot of parameters, but for a decently sized neural network, this isn't much. Notice that the number of parameters scales linearly with parameters like the sequence length (it would be exponential if we had used n-grams!). It seems like our neural NLP model is relatively compact!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: RNN Warmup\n",
    "\n",
    "Now we'll follow the same approach to better understand RNNs. \n",
    "\n",
    "Note that when we build an RNN architecture, the hidden state size is a hyperparameter, just like it was for feedforward networks. Given an embedding size and a hidden size, calculate the number of parameters needed to represent W_e."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_parameters_W_e(embedding_size, hidden_size):\n",
    "    ### YOUR CODE HERE\n",
    "    return embedding_size * hidden_size\n",
    "    ### END CODE\n",
    "    \n",
    "print(\"Number of parameters for W_e with embedding size 100 and hidden size 50:\", num_parameters_W_e(100, 50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output**\n",
    "\n",
    "Number of parameters for W_e with embedding size 100 and hidden size 50: 5000\n",
    "    \n",
    "Now do the same thing for W_h:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_parameters_W_h(embedding_size, hidden_size):\n",
    "    ### YOUR CODE HERE\n",
    "    return hidden_size * hidden_size\n",
    "    ### END CODE\n",
    "print(\"Number of parameters for W_h with embedding size 100 and hidden size 50:\", num_parameters_W_h(100, 50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output**\n",
    "\n",
    "Number of parameters for W_h with embedding size 100 and hidden size 50: 2500\n",
    "    \n",
    "Now calculate the number of parameters needed to calculate the next hidden state from the previous one and the current input embedding. This involves W_e, W_h, and b_1. You will find your previous functions helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_parameters_hidden(embedding_size, hidden_size):\n",
    "    ### YOUR CODE HERE\n",
    "    return num_parameters_W_e(embedding_size, hidden_size) + num_parameters_W_h(embedding_size, hidden_size) + hidden_size\n",
    "    ### END CODE\n",
    "print(\"Number of hidden params with embedding size 100 and hidden size 50:\", num_parameters_hidden(100, 50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output**\n",
    "\n",
    "Number of hidden params with embedding size 100 and hidden size 50: 7550\n",
    "    \n",
    "Now calculate the number of parameters needed to calculate the output at the final timestep. Note that the output is O = softmax(U\\*h + b_2), where h is the hidden state at the final timestep. You'll need vocabulary size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_parameters_output(embedding_size, hidden_size, vocabulary_size):\n",
    "    ### YOUR CODE HERE\n",
    "    return vocabulary_size * hidden_size + vocabulary_size\n",
    "    ### END CODE\n",
    "print(\"Number of output params with embedding size 100 hidden size 50, vocab size 100:\", num_parameters_output(100, 50, 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output**\n",
    "\n",
    "Number of output params with embedding size 100 hidden size 50, vocab size 100: 5100\n",
    "    \n",
    "Now combine everything together to get the number of parameters for our complete RNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_parameters_RNN(embedding_size, hidden_size, vocabulary_size):\n",
    "    ### YOUR CODE HERE\n",
    "    return num_parameters_hidden(embedding_size, hidden_size) + num_parameters_output(embedding_size, hidden_size, vocabulary_size)\n",
    "    ### END CODE\n",
    "print(\"Number of params with embedding size 100, hidden size 50, vocabulary size 100:\", num_parameters_RNN(100, 50, 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output**\n",
    "\n",
    "Number of params with embedding size 100, hidden size 50, vocabulary size 100: 12650\n",
    "    \n",
    "Notice that even though our model has similar hyperparameters to our original feedforward network, we have significantly fewer parameters because of weight-sharing! This gives us more room to build complex models without making computation too expensive. Also notice that sequence length did not factor into our calculations: we can run any length sequence through our RNN to make a prediction, and we don't need extra parameters for this!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Generating Fake News\n",
    "\n",
    "Now we'll apply our knowledge of RNNs to build a language model for fake news! Besides being a fun way to test our new models against real world data, building a language model for fake news is one way to show how AI can be used for bad. We'll take our language model and use it to generate new fake news that the world has never seen before.\n",
    "\n",
    "Let's start by loading up our dataset. Remember that this is a subset of a dataset scraped from websites tagged as fake news providers by OpenSources. The full dataset can be found [here](https://www.kaggle.com/mrisdal/fake-news/version/1#). \n",
    "\n",
    "It's just a .txt file containing a bunch of articles concatenated together. Run the below cell to view the first couple of articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with io.open('resources/fake.txt', encoding='utf-8') as f:\n",
    "    articles_raw = f.read()\n",
    "    articles_split = re.split(\"<a>\", articles_raw)[1:]\n",
    "    articles = [a[:-6].strip() for a in articles_split]\n",
    "print(articles[0])\n",
    "print(\"\\n\")\n",
    "print(articles[1])\n",
    "print(\"\\n\")\n",
    "print(articles[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The way we will build our language model will be slightly different from the way presented in class. Instead of predicting the next word at each time step, our language model will predict the next character. Each step in the sequence will be a character input. This means we will be generating fake news character by character! Surprisingly, it will still look decently plausible. Note that instead of passing word embeddings at each time step as inputs for our RNN, we will be passing in one-hot vectors representing the current character. \n",
    "\n",
    "Also remember that, as mentioned in class, the way we train our model is to cut up our full corpus into manageable chunks and have our model predict the next character after each chunk. We will be using chunks of size 40 characters.\n",
    "\n",
    "This means that our *X* will have dimensionality (number_of_chunks, 40, number_of_characters). This means that for each chunk, and for each of 40 characters in each chunk, we will have a one-hot vector of size 40 representing the character in that position in the chunk. Our *y* will have dimensionality (number_of_chunks, number_of_characters). For each chunk, we have a one-hot vector representing the next character after the corresponding chunk.\n",
    "\n",
    "Run the below code to get *X* and *y* as described above. The code that does this can be found in utils.py, if you're curious how this was done. The code also gives us *char_indices* and *indices_char*, which are just dictionaries mapping from character to its one-hot index and vice-versa. We'll need these later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, char_indices, indices_char = utils.get_X_y(articles_raw)\n",
    "\n",
    "print(\"Shapes:\", X.shape, y.shape)\n",
    "\n",
    "number_of_chunks, chunk_length, number_of_characters = X.shape\n",
    "\n",
    "print(number_of_chunks, chunk_length, number_of_characters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now just build a Keras model to fit this data. Your model should be an LSTM (non-bidirectional). Feel free to add multiple layers, but note that this will make your model more computationally expensive (if you do this, ensure all LSTM layers before the last one have the *return_sequences* flag set to True). The output layer should be fully-connected (Dense) with *number_of_characters* neurons and a softmax activation, since we are doing classification among this many classes. You'll find the documentation [here](https://keras.io/layers/recurrent/) helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "### YOUR CODE HERE\n",
    "model.add(LSTM(128, input_shape=(chunk_length, number_of_characters), return_sequences=True))\n",
    "model.add(LSTM(64))\n",
    "model.add(Dense(number_of_characters))\n",
    "model.add(Activation('softmax'))\n",
    "### END CODE\n",
    "\n",
    "optimizer = RMSprop(lr=0.0005)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You could try to train this model, but it will take several hours. We trained a version of this model for you so you can see the results. Run the below cell to load the pretrained model and a summary of its architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model = load_model('resources/pretrained_model.h5')\n",
    "pretrained_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the below cell to sample text from the language model. This code picks a random chunk in our dataset and feeds it into our language model to predict the next character and repeats. The diversity refers to the tendency of the model to sample less likely characters for the sake of producing more novel text. Higher diversity results in more novel text, but it often makes less sense. Try running the below cell a few times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.sample_from_model(pretrained_model, articles_raw, char_indices, indices_char, chunk_length, number_of_characters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you squint, you may be able to see a conspiracy theory taking shape! For more fun with this, supply your own initial chunk (seed). The initial chunk can be at most 40 characters (otherwise it will be cut off). If it is shorter, it will be left-padded with whitespace. The seed can only contain characters in the dataset! Note that since the seed isn't coming from the dataset, the result may be less coherent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.sample_from_model(pretrained_model, articles_raw, char_indices, indices_char, chunk_length, number_of_characters, seed=\"Hillary Clinton is under inv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's plenty of room for improvement with this model. For the homework, you'll develop a metric to quantitatively evaluate language models, optionally train your own model, and evaluate it against the metric.\n",
    "\n",
    "You can find the actual script we used to train this model in the resources folder. You can play around with hyperparameters, create more compelling demonstrations, or do anything else you want!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
