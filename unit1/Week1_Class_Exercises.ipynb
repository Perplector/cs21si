{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 1 Exercises\n",
    "\n",
    "Today we will explore gender bias in natural language processing. We will learn about our first models to probe gender bias in word vectors. As a reminder, word vectors are a machine's representation of a word, learned from reading a large corpus of text to understand the context that words are used in. For example, since the words \"good\" and \"great\" are used in similar contexts, they have similar word vectors!\n",
    "\n",
    "These kinds of word vectors are used in everything from Google Search to Spotify recommendations, so if they are biased, this is a major problem.\n",
    "\n",
    "Today we will be using GloVe vectors, which are a standard type of word vector used in a variety of real-world applications. These word vectors were trained on 6 billion word tokens, sourced from Wikipedia 2014 + Gigaword5. If you're interested you can find more information [here](https://nlp.stanford.edu/projects/glove/).\n",
    "\n",
    "Run the below cell by highlighting it and typing Shift+Enter. This will import the required packages and download the GloVe vectors, which will take a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext.vocab as vocab\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "\n",
    "VEC_SIZE = 300\n",
    "glove = vocab.GloVe(name='6B', dim=VEC_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1\n",
    "Below we have included a short helper function that retrieves the word vector for a given word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_vector(word):\n",
    "    return glove.vectors[glove.stoi[word]].numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe the results of this helper function below. Notice that we are outputting a numpy array of dimensionality (300,). This means that the output is a 300-dimensional vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good = get_word_vector('good') # get the word vector for 'good'\n",
    "print(good.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, use the above vector and the word vector for 'great' to determine the cosine similarity between 'good' and 'great'. Do the same for 'good' and 'human' (two words that are less similar). You'll need *np.linalg.norm* and *np.dot*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "great = get_word_vector('great') # YOUR CODE HERE\n",
    "human = get_word_vector('human') # YOUR CODE HERE\n",
    "def compute_cosine_similarity(a, b):\n",
    "    # YOUR CODE HERE:\n",
    "    return None\n",
    "    # END CODE\n",
    "\n",
    "print(\"Good-great similarity %f\" % compute_cosine_similarity(good, great))\n",
    "print(\"Good-human similarity %f\" % compute_cosine_similarity(good, human))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output:**\n",
    "\n",
    "Good-great similarity 0.641005\n",
    "\n",
    "Good-human similarity 0.313640\n",
    "\n",
    "Now, use our helper function to retrieve the \"gender vector\", or the vector representing 'woman' minus the vector representing 'man' (woman - man). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "gender_vector = None\n",
    "# END CODE\n",
    "print('First value of gender vector: %f ' % gender_vector[0])\n",
    "print('Shape of gender vector: ', gender_vector.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output:**\n",
    "\n",
    "First value of gender vector: -0.220370 \n",
    "\n",
    "Shape of gender vector:  (300,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now fill in the below function that computes linear regression on any word. Use the gender_vector to provide weights (*w*), and do not use a bias term (*b*). You'll need our helper function *get_word_vector* and *np.dot*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_linear_regression(word):\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    return None\n",
    "    # END CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check to make sure your model matches the expected output for 'programmer':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_linear_regression('programmer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output:**\n",
    "-1.0347012"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feel free to play around with the model by changing the input word in the below cell! How does the score for 'programmer' compare to the score for 'nurse'? For 'homemaker'? What does this tell us about our word vectors?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_linear_regression('homemaker')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2\n",
    "\n",
    "Now we will build a more sophisticated logistic regression model to make predictions on our word vectors. This model will actually learn the weights (*w*) and bias (*b*) by itself! It will also output a probability between 0 and 1 that a word is associated with females. (1 represents a word that is very 'female' according to our word vectors, 0 represents a word that is not) \n",
    "\n",
    "All we will do is tell this model that 1 represents female and 0 represents male, and, alarmingly, bias from our word vectors will transfer to our model.\n",
    "\n",
    "For these in-class exercises, you will build this model and learn how to train it. For homework, you will actually train it and see the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a warmup, fill in the below function to calculate the sigmoid of a scalar value. Use *np.exp* instead of python's built-in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    return None\n",
    "    # END CODE\n",
    "print(\"sigmoid(0.5) is %f\" % sigmoid(0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output:**\n",
    "\n",
    "sigmoid(0.5) is 0.622459\n",
    "\n",
    "Next, fill in the below function to compute logistic regression on a word given weights and bias. Note that you are not training the model yet, just computing what is known as the \"forward pass\". This should look similar to your *compute_linear_regression* function, but you are using the weights and bias given instead of the *gender_vector*, and you are using sigmoid to produce the final output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_logistic_regression(word, weights, bias):\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    return None\n",
    "    # END CODE\n",
    "\n",
    "np.random.seed(42)\n",
    "rand_weights = np.random.randn(VEC_SIZE)\n",
    "rand_bias = np.random.rand()\n",
    "print(\"Predicted output: %f\" % compute_logistic_regression('hello', rand_weights, rand_bias))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output:**\n",
    "\n",
    "Predicted output: 0.000089\n",
    "\n",
    "Don't read too much into this output, it was randomly generated. \n",
    "\n",
    "Great, now how do we actually train *weights* and *bias*? For our first and only \"random guess\", we can initialize *weights* randomly and *bias* as 0. We then update both of these away from the direction of the gradient with respect to loss, for each training example. Since the gradient is the direction of maximum 'upward slope', moving away from the gradient minimizes the loss. Note that we loop over the training set *NUM_EPOCHS*=1000 times so that the model has more time to learn the training set.\n",
    "\n",
    "You will need *np.random.randn* (see previous cell for usage example), *np.log* (to calculate loss), and our helper functions. \n",
    "\n",
    "Don't worry if you don't finish this in class, we expect this!\n",
    "\n",
    "**Some hints:**\n",
    "\n",
    "Initialize *weights* using *np.random.randn* and bias to 0.\n",
    "\n",
    "You are first computing the prediction *pred* (the same thing as y_hat), then using this to compute the loss, then computing the gradients, then using them to make weight updates.\n",
    "\n",
    "When computing loss, accumulate loss over each epoch using the '+=' operator, so the final loss printed per epoch is the sum of the losses for each training example.\n",
    "\n",
    "Notation note: dw and db are short for the partial derivatives of the loss with respect to w and b, respectively.\n",
    "\n",
    "Use the *LEARNING_RATE* provided to update the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_logistic_regression(training_data, NUM_EPOCHS=1000, LEARNING_RATE=0.001):\n",
    "    np.random.seed(42)\n",
    "    # YOUR CODE HERE - initialize weights and bias\n",
    "    weights = None \n",
    "    bias = None\n",
    "    # END CODE\n",
    "    \n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        loss = 0\n",
    "        for example in training_data:\n",
    "            x, y = example\n",
    "            # YOUR CODE HERE\n",
    "            # Fill in values for pred (another name for y_hat) and the loss for the current example\n",
    "            pred = None\n",
    "            loss += None\n",
    "            \n",
    "            # Calculate gradients here\n",
    "            \n",
    "            # Update weights and bias here\n",
    " \n",
    "            # END CODE\n",
    "        if epoch % 100 == 0:\n",
    "            print(\"Epoch %d, loss = %f\" % (epoch, loss))   \n",
    "    return weights, bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By looping through each test example and computing gradients for each individually, we are performing what is called Stochastic Gradient Descent (SGD). We'll learn more about this in the coming weeks.\n",
    "\n",
    "Test your implementation to see if its results match ours:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toy_examples = [('boy', 0), ('girl', 1)]\n",
    "weights, bias = fit_logistic_regression(toy_examples)\n",
    "print(\"First value in weights is %f\" % weights[0])\n",
    "print(\"Bias is %f\" % bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output:**\n",
    "\n",
    "Epoch 0, loss = -3.843859\n",
    "\n",
    "Epoch 100, loss = -1.420667\n",
    "\n",
    "Epoch 200, loss = -1.006021\n",
    "\n",
    "Epoch 300, loss = -0.816709\n",
    "\n",
    "Epoch 400, loss = -0.680780\n",
    "\n",
    "Epoch 500, loss = -0.578991\n",
    "\n",
    "Epoch 600, loss = -0.500879\n",
    "\n",
    "Epoch 700, loss = -0.439593\n",
    "\n",
    "Epoch 800, loss = -0.390541\n",
    "\n",
    "Epoch 900, loss = -0.350585\n",
    "\n",
    "First value in weights is 0.396410\n",
    "\n",
    "Bias is 0.083128\n",
    "\n",
    "You've just built a working logistic regression model from scratch. We'll see week 3 that this is actually equivalent to a 1-layer neural network!\n",
    "\n",
    "In your homework assignment, you will put this model into use. Unfortunately, you'll quickly find that it picks up on the implicit gender biases found in the word vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
