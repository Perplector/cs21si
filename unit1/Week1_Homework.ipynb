{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 1 Homework\n",
    "\n",
    "In this homework, we will apply the logistic regression model we built in class and see that gender biases transfer from the word vectors to our complete model. All we will do is tell the model that 1 is for female and 0 is for male, and it will learn harmful gender stereotypes (from the word vectors, which in turn learned from Wikipedia, a dataset containing implicit bias).\n",
    "\n",
    "This is different from the linear regression case because we are training a model that is making predictions on the word vector, rather than fixing the weights as some vector. This means that gender biases in word vectors are so prevalent that any model built on top of them (just about any model for natural language processing) is at risk for including implicit bias. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext.vocab as vocab\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "\n",
    "VEC_SIZE = 300\n",
    "glove = vocab.GloVe(name='6B', dim=VEC_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we define a couple of helpful helper functions, including our *get_word_vector* from before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_vector(word):\n",
    "    return glove.vectors[glove.stoi[word]].numpy()\n",
    "\n",
    "def read_train_examples():\n",
    "    with open('resources/train.txt', 'r') as f:\n",
    "        raw_text = f.read()\n",
    "        lines = raw_text.split('\\n')\n",
    "        examples = [line.split() for line in lines]\n",
    "        examples = [(line[0], int(line[1])) for line in examples]\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, copy-and-paste your complete *sigmoid*, *fit_logistic_regression*, and *compute_logistic_regression* functions from the in-class exercises. If you didn't have time in class to finish these functions, you should do that now. We will use them below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    # YOUR CODE HERE\n",
    "    return None\n",
    "    # END CODE\n",
    "\n",
    "def fit_logistic_regression(training_data, NUM_EPOCHS=1000, LEARNING_RATE=0.001):\n",
    "    # YOUR CODE HERE\n",
    "    return None\n",
    "    # END CODE\n",
    "\n",
    "\n",
    "def compute_logistic_regression(word, weights, bias):\n",
    "    # YOUR CODE HERE\n",
    "    return None\n",
    "    # END CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, use *read_train_examples* to read examples from 'resources/train.txt'. Browse the training examples and note that each contains a word with an appropriate, non-problematic gender association. By training our model on these words, we are effectively telling it that 'female' should result in a '1' output and 'male' should result in a '0' output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE\n",
    "examples = None\n",
    "### END CODE\n",
    "print(examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then use *fit_logistic_regression* to get weights and a bias for this data. We will use these to make predictions on unseen data below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE\n",
    "\n",
    "### END CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we define a helper function that \"pretty-prints\" the outputs of our model. Make sure you understand how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_test_output(test_examples, weights, bias):\n",
    "    for test_example in test_examples:\n",
    "        pred = compute_logistic_regression(test_example, weights, bias)\n",
    "        print(\"%s is %s\" % (test_example, 'male' if pred < .5 else 'female'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a sanity check, let's see what happens if we print output for pronouns that were in the training data. Testing on train data is a common technique to debug models and make sure everything is working properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_test_output(['she', 'he'], weights, bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected output:**\n",
    "\n",
    "she is female\n",
    "\n",
    "he is male\n",
    "\n",
    "Now, let's see what happens if we ask the model about common professions. Note that we did not tell the model about these gender-neutral professions, but it is able to make predictions on them anyway because we still have word vectors for them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_test_output(['nurse', 'homemaker', 'carpenter', 'surgeon', 'doctor', 'artist', \n",
    "                   'engineer', 'entrepreneur', 'genius', 'intellectual', 'chef', 'cook', \n",
    "                   'maid', 'teacher', 'boss', 'manager', 'founder', 'programmer'], weights, bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the model seems to capture common gender stereotypes about professions. We never explicitly told the model any of these things during training! The model seems to get the \"wrong\" answer only for 'programmer'.\n",
    "\n",
    "This is alarming, since almost all natural language processing models are built on top of these word vectors or ones like them. If we can accidentally built a model that displays gender bias this easily, so can people working at Google, Facebook, Microsoft, etc. We will continue our exploration of bias in word vectors during week 3. In the meantime, if you're interested in looking at techniques to debias word vectors, [here is a great, approachable paper on just that](https://arxiv.org/abs/1607.06520)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
