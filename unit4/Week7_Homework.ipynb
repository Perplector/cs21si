{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 7 Homework\n",
    "\n",
    "This week, we'll be talking about adversarial examples. Basically, we fool the neural network into thinking that an image which is abnormal is totally healthy--super scary stuff!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os, scipy.ndimage, scipy.misc\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import util \n",
    "\n",
    "import keras\n",
    "import keras.backend as K\n",
    "from keras.models import load_model\n",
    "from keras.objectives import binary_crossentropy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Model\n",
    "\n",
    "First, we load a pre-trained model on this task. Run the code below to load it. This is the model we are going to break."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('resources/trained_model.h5')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pick a Test Case\n",
    "\n",
    "We now pick an image that we are going to play with. This image is an abnormal sample from the test set--in other words, we pick an image from the test set which shows an unhealthy lession on the skin of the patient. Notice that the output the model predicts is quite close to 1, so it is very confident that this is unhealthy. We will make the network believe this is healthy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = 'resources/addi/test/abnormal/IMD024.png'\n",
    "img = scipy.ndimage.imread(img_path).astype(float)\n",
    "img = scipy.misc.imresize(img, (150, 150, 3))[np.newaxis, :, :, :]/255.0\n",
    "\n",
    "plt.imshow(np.squeeze(img))\n",
    "plt.show()\n",
    "\n",
    "print(\"Prediction using this image: \", model.predict(img)[0, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Untargeted Noise\n",
    "\n",
    "Before we dive into how adversarial examples work, try this out: just intialize some random noise and add it to our above image. Then, see how the model does. Mess around with the scaling/shifting factor of the noise to try to get the score below 0.5 (making the model think it is healthy). This is called an untargeted attack--we don't know what output the model will give this noisy input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale, shift = .2, 0\n",
    "noise = np.random.rand(1, 150, 150, 3) * scale + shift\n",
    "\n",
    "noisy_img = np.clip(img + noise, 0, 1)\n",
    "plt.imshow(np.squeeze(noisy_img))\n",
    "plt.show()\n",
    "\n",
    "print(\"Prediction using this image: \", model.predict(noisy_img)[0, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Targeted Noise\n",
    "\n",
    "Now for the interesting stuff: we will change the image until we fool the model. How does this work? It's quite simple: we compute the gradient of the normal class score with respect to the input image. This tells us how much to change the input image to maximize the normal class score--i.e. how do we change the unhealthy image to make it look healthy. That's it!\n",
    "\n",
    "Notice how, as we execute this code to do that, the score decreases. Mess with the pertubation amount to try to get the model output below a 0.5 on this input. This means that the model now thinks that the unhealthy image is healthy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pertubation_amount = 0.1\n",
    "\n",
    "fooling_img = img.copy()\n",
    "for _ in range(10):\n",
    "    # We create a function that gets the gradients of BCE Loss with respect to the input image\n",
    "    y_true = K.placeholder((1, 1))\n",
    "    loss = K.mean(binary_crossentropy(y_true, model.output))\n",
    "    get_grads = K.function([model.input, y_true], K.gradients(loss, model.input))\n",
    "\n",
    "    # We pass in the input image and the label we want the model to output (0)\n",
    "    grad = get_grads([fooling_img, [[0]]])[0]\n",
    "    fooling_img = np.clip(fooling_img - pertubation_amount*grad, 0, 1)\n",
    "    plt.imshow(fooling_img[0])\n",
    "    plt.show()\n",
    "    \n",
    "    # If our model thinks the image is normal, stop right here\n",
    "    pred = model.predict(fooling_img)[0, 0]\n",
    "    print(pred)\n",
    "    if pred <= 0.5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the \"Fooling\" Image\n",
    "\n",
    "Let's see how this adversarial example looks!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.squeeze(fooling_img))\n",
    "plt.show()\n",
    "pred = model.predict(fooling_img)[0, 0]\n",
    "print(\"Prediction using this image: \", pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, this looks exactly like the input image! However, our model thinks that this is healthy when it is clearly not. In just a couple of steps, you were able to take an unhealthy image and make it look healthy to the network. That is super scary! This is something to keep in mind as you design your high-stake ML algorithms.\n",
    "\n",
    "If you want to know more about adversarial examples and how we can protect against them, check out [this paper on the topic](https://arxiv.org/pdf/1712.07107.pdf). For a more casual read, check out [this neat blog post](https://www.anishathalye.com/2017/07/25/synthesizing-adversarial-examples/) on the topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
