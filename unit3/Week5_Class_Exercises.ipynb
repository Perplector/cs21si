{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 5 Exercises\n",
    "\n",
    "Today, we'll be exploring deep neural networks and applying them to data related to the COMPAS algorithm. I got this data [here](https://github.com/propublica/compas-analysis), in case you're interested in exploring it on your own!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the Environment\n",
    "\n",
    "Clone the `unit3` repository from GitHub and install dependencies in requirements.txt [(instructions here, just in case)](http://web.stanford.edu/class/cs21si/setup.html).\n",
    "\n",
    "Run any code below by highlighting it and hitting `Shift + Enter`. Import the libraries below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, BatchNormalization\n",
    "from keras import regularizers\n",
    "from keras import backend as K\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "np.random.seed(1337)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "filename = \"resources/compas-scores.csv\"\n",
    "\n",
    "# load data\n",
    "data = pd.read_csv(filename, header = 0)\n",
    "\n",
    "# select fields we want\n",
    "fields_of_interest = ['name', 'sex', 'age', 'race', 'priors_count', 'c_charge_desc', \n",
    "                      'v_decile_score', 'decile_score', 'is_violent_recid', 'is_recid']\n",
    "data = data[fields_of_interest]\n",
    "data.columns = ['name', 'sex', 'age', 'race', 'num_priors', 'charge', \n",
    "                'violence_score', 'recidivism_score', 'violence_true', 'recidivism_true']\n",
    "\n",
    "# remove records with missing scores\n",
    "data = data.loc[(data.violence_score != -1) & (data.recidivism_score != -1)]\n",
    "data = data.loc[(data.violence_true != -1) & (data.recidivism_true != -1)]\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jupyter Exercise 1: Visualize the Data\n",
    "\n",
    "With the increasing availability of rich data sets encoding several features, it's difficult to extract useful knowledge just by looking at the numbers. This is where data visualizations come in. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# races of interest\n",
    "races = ['African-American', 'Caucasian', 'Hispanic']\n",
    "colors = ['magenta', 'yellow', 'cyan']\n",
    "\n",
    "#########################################################\n",
    "# Counts number of people per race. Plots as bar graph.\n",
    "#########################################################\n",
    "def plot_racial_distrib():\n",
    "    race_indices = [1,2,3]\n",
    "    race_population = [(data.loc[data['race'] == race].shape[0]) for race in races]\n",
    "\n",
    "    plt.bar(race_indices, race_population, align='center', color = colors)\n",
    "    plt.xticks(race_indices, races)\n",
    "    plt.title('Racial Distribution of COMPAS Defendants')\n",
    "    plt.rcParams[\"figure.figsize\"] = [15, 5]\n",
    "    plt.show()\n",
    "\n",
    "#########################################################\n",
    "# Plots input feature by race as bar graph.\n",
    "#########################################################\n",
    "def plot_feature_by_race(feature, normalized = True):\n",
    "    # bar chart parameters\n",
    "    width = 0.25\n",
    "    groups = sorted(data[feature].unique(), key=int)\n",
    "    deciles = np.arange(1, len(groups) + 1)\n",
    "    # deciles = np.arange(1, 11) # for each of the 10 scores\n",
    "    \n",
    "    if normalized:\n",
    "        race_population = [(data.loc[data['race'] == race].shape[0]) for race in races]\n",
    "    else:\n",
    "        race_population = [1 for race in races]\n",
    "    \n",
    "    race_bars = []\n",
    "    for i in range(len(races)):\n",
    "        race_data = data.loc[data['race'] == races[i]]\n",
    "        bar = plt.bar(deciles + (i-1)*width, race_data[feature].value_counts()/race_population[i], \n",
    "                      width, color = colors[i])\n",
    "        race_bars.append(bar)\n",
    "\n",
    "    plt.title('COMPAS ' + feature + ' by Race')\n",
    "    plt.xticks(deciles + width / 2, groups)\n",
    "    plt.legend(tuple(bar for bar in race_bars), tuple(races))\n",
    "    plt.rcParams[\"figure.figsize\"] = [15, 5]\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your task:** Using the functions defined above, plot the racial distribution of defendants in our database. Also plot the violence and recidivism COMPAS scores by race. You should see that African-Americans are disproportionately assigned higher recidivism and violence scores, compared to Caucasians and Hispanics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE:\n",
    "\n",
    "# END CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up the Data a Little More"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# convert strings to numerical values\n",
    "races = ['African-American', 'Asian', 'Caucasian', 'Hispanic', 'Native American', 'Other']\n",
    "sex_classes = {'Male': 0, 'Female' : 1}\n",
    "race_classes = {races[i]: i for i in range(len(races))}\n",
    "\n",
    "# 'Other': 0, 'Caucasian': 1, 'African-American': 2, 'Hispanic': 3, 'Asian': 4, 'Native American': 5}\n",
    "data['sex'] = data['sex'].apply(lambda x: sex_classes[x])\n",
    "data['race'] = data['race'].apply(lambda x: race_classes[x])\n",
    "# threshold and binarize scores\n",
    "data['violence_score'] = data['violence_score'].apply(lambda x: 0 if x <= 5 else 1)\n",
    "data['recidivism_score'] = data['recidivism_score'].apply(lambda x: 0 if x <= 5 else 1)\n",
    "print(data)\n",
    "\n",
    "# convert pandas dataframe to numpy array for easier processing\n",
    "data = data.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jupyter Exercise 2: Partition into Train and Test Sets\n",
    "\n",
    "The scores above have been thresholded and binarized (values > 5 are now 1, others are 0). We will focus on predicting recidivism scores. This is now a binary classification problem where our inputs are sex, age, race, and number of prior convictions. Our output is 1 (high risk of recidivism) or 0 (low risk of recidivism). The entire dataset has already been split into inputs (X) and outputs (y). \n",
    " \n",
    "**Your task:** Partition the dataset into two sets: train (80%) and test (20%). The validation split will happen later. Assume the records have already been shuffled. Use the function `get_rows()` defined below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# split into input (X) and output (Y) variables\n",
    "X = data[:,1:5] # sex, age, race, num_priors\n",
    "y = data[:,7] # recidivism_score\n",
    "\n",
    "num_train = math.ceil(X.shape[0]*0.8)\n",
    "num_test = math.floor(X.shape[0]*0.2)\n",
    "\n",
    "#########################################################\n",
    "# Returns the specified records of a given array, from\n",
    "# row_start to row_start + num_rows - 1 (inclusive).\n",
    "#########################################################\n",
    "def get_rows(dataset, row_start, num_rows):\n",
    "    return dataset[row_start:row_start + num_rows]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE:\n",
    "\n",
    "# END CODE\n",
    "\n",
    "num_classes = 2\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "print(X_train.shape[0], 'records in train set')\n",
    "print(X_test.shape[0], 'records in test set')\n",
    "print(X.shape[0], 'records in total')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jupyter Exercise 3: Create a Baseline Model\n",
    "\n",
    "**Your task:** At this point, we're ready to create a basic neural network classifier using Keras. Your model should have 4 layers, of sizes 50, 100, 50, and `num_classes`. All layers, save the last one, should use a ReLU activation function. The last layer should use softmax. Check this [guide](https://keras.io/getting-started/sequential-model-guide/) out to get started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "epochs = 15\n",
    "learning_rate = 5e-2\n",
    "\n",
    "#########################################################\n",
    "# Initializes baseline neural network with 4 layers.\n",
    "# ReLU and Softmax activations. Cross-entropy loss.\n",
    "#########################################################\n",
    "def baseline_classifier(learning_rate):\n",
    "    # create model\n",
    "    model_baseline = Sequential()\n",
    "\n",
    "    # YOUR CODE HERE:\n",
    "    \n",
    "    # END CODE\n",
    "\n",
    "    # compile model\n",
    "    sgd = keras.optimizers.SGD(lr = learning_rate)\n",
    "    model_baseline.compile(loss = keras.losses.categorical_crossentropy, \n",
    "                  optimizer = sgd, metrics=['accuracy'])\n",
    "    \n",
    "    return model_baseline\n",
    "\n",
    "model_baseline = baseline_classifier(learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jupyter Exercise 4: Train and Evaluate the Baseline Model\n",
    "\n",
    "**Your task:** Use the `eval()` function below to train and evaluate our baseline model. The return value of `eval()` is a tuple of loss and accuracy. Print both of these. In `eval()`, feel free to change the value of the `verbose` parameter. When `verbose = 0`, no information is printed. When it's 5, a lot of detailed information about the training process gets printed. Your test accuracy for this basic model should be around 37%.\n",
    "\n",
    "Note that a parameter is `model.fit()` is `validation_split`. This takes a float from 0 to 1, representing the percentage of the training set to use for validation. Why do we need a validation set? More than training performance, we are interested in how our model does on unseen data. We can split data into only training and test. But if we then optimize our model using results from the test set, our test set can no longer be considered unseen data. As a workaround, we split our dataset into train, validation, and test. This way, we can optimize on our validation set, and only touch our test set at the very end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#########################################################\n",
    "# Trains and evaluates given model. Returns loss and \n",
    "# accuracy.\n",
    "#########################################################\n",
    "def eval(model, verb = 2):\n",
    "    # fit the model\n",
    "    model.fit(X_train, y_train, \n",
    "              epochs = epochs, \n",
    "              batch_size = batch_size,          \n",
    "              validation_split = 0.1,\n",
    "              verbose = verb,\n",
    "              shuffle = False)\n",
    "    \n",
    "    # evaluate the model\n",
    "    scores = model.evaluate(X_test, y_test)\n",
    "    \n",
    "    return scores\n",
    "\n",
    "# YOUR CODE HERE:\n",
    "\n",
    "# END CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jupyter Exercise 5: Introduce Regularization\n",
    "\n",
    "Sometimes our train accuracies are consistently higher than our test accuracy.  We might also see train losses that continue to decrease while the validation losses hit a minimum then increase. Both of these things are indicators of overfitting and poor model generalization. Regularization is a way to fix this. Regularization reduces overfitting by adding a penalty to the loss function. By adding this penalty, the model is trained such that it does not learn interdependent sets of features weights.\n",
    "\n",
    "**Your task:** Add L2 regularization to each hidden layer of our model. You may want to refer to the [Keras documentation about regularizers](https://keras.io/regularizers/). You can see up to a 30% gain in test accuracy after adding in regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reg_strength = 0.15\n",
    "\n",
    "#########################################################\n",
    "# Initializes neural network with L2 regularization.\n",
    "#########################################################\n",
    "def regularized_classifier(learning_rate, reg_strength):\n",
    "    # create model\n",
    "    model_regularized = Sequential()\n",
    "\n",
    "    # YOUR CODE HERE:\n",
    "\n",
    "    # END CODE\n",
    "    \n",
    "    # compile model\n",
    "    sgd = keras.optimizers.SGD(lr = learning_rate)\n",
    "    model_regularized.compile(loss = keras.losses.categorical_crossentropy, \n",
    "                  optimizer = sgd, metrics=['accuracy'])\n",
    "    \n",
    "    return model_regularized\n",
    "\n",
    "model_regularized = regularized_classifier(learning_rate, reg_strength)\n",
    "\n",
    "loss, acc = eval(model_regularized, verb = 0)\n",
    "print('\\n\\nTest loss:', loss)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jupyter Exercise 6: Introduce Dropout\n",
    "\n",
    "Dropout is another regularization technique to prevent over-fitting. Dropout randomly selects neurons to ignore during training. In other words, they are “dropped-out” randomly. This means that their contribution to the activation of downstream neurons is temporally removed on the forward pass and any weight updates are not applied to the neuron on the backward pass. \n",
    "\n",
    "**Your task:** Remove the regularization, and this time around, add dropout for the input layer. You may want to refer to the [Keras documentation about Dropout layer](https://keras.io/layers/core/). You may see up to a 5% increase in test accuracy compared to the regularization model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dropout_strength = 0.1\n",
    "\n",
    "#########################################################\n",
    "# Initializes neural network with dropout.\n",
    "#########################################################\n",
    "def dropout_classifier(learning_rate, dropout_strength):\n",
    "    # create model\n",
    "    model_dropout = Sequential()\n",
    "\n",
    "    # YOUR CODE HERE:\n",
    "\n",
    "    # END CODE\n",
    "    \n",
    "    # compile model\n",
    "    sgd = keras.optimizers.SGD(lr = learning_rate)\n",
    "    model_dropout.compile(loss = keras.losses.categorical_crossentropy, \n",
    "                  optimizer = sgd, metrics=['accuracy'])\n",
    "    \n",
    "    return model_dropout\n",
    "\n",
    "model_dropout = dropout_classifier(learning_rate, dropout_strength)\n",
    "\n",
    "loss, acc = eval(model_dropout, verb = 0)\n",
    "print('\\n\\nTest loss:', loss)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jupyter Exercise 7: Introduce Batch Normalization\n",
    "\n",
    "Batch Normalization is used to help remedy “covariate shifts”, or changes in the distribution of function’s domain.\n",
    "They help keep activations in the zero-mean unit-variance range. You might use batch normalization for the same reasons that you normalized the entire dataset initially.\n",
    "\n",
    "**Your task:** For our final exercise, keep dropout. Add batch normalization after the first dense layer. You may want to refer to the [Keras documentation about the BatchNorm layer](https://keras.io/layers/normalization/). You can see up to a 5% increase in test accuracy compared to the dropout-only model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#########################################################\n",
    "# Initializes neural network with dropout.\n",
    "#########################################################\n",
    "def batchnorm_classifier(learning_rate, dropout_strength):\n",
    "    # create model\n",
    "    model_batchnorm = Sequential()\n",
    "\n",
    "    # YOUR CODE HERE:\n",
    "\n",
    "    # END CODE\n",
    "    \n",
    "    # compile model\n",
    "    sgd = keras.optimizers.SGD(lr = learning_rate)\n",
    "    model_batchnorm.compile(loss = keras.losses.categorical_crossentropy, \n",
    "                  optimizer = sgd, metrics=['accuracy'])\n",
    "    \n",
    "    return model_batchnorm\n",
    "\n",
    "model_batchnorm = batchnorm_classifier(learning_rate, dropout_strength)\n",
    "\n",
    "loss, acc = eval(model_batchnorm, verb = 0)\n",
    "print('\\n\\nTest loss:', loss)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
