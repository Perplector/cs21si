{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 6 Homework\n",
    "\n",
    "This week's homework will help you build up your deep learning skills and apply them to the healthcare space. Specifically, we'll predicting whether a breast mass is benign or malignant. I got this data [here](https://www.kaggle.com/uciml/breast-cancer-wisconsin-data), in case you're interested in exploring it on your own! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the Environment\n",
    "\n",
    "Do a `git pull` from the `unit3` GitHub repository, and install dependencies in requirements.txt [(instructions here, just in case)](http://web.stanford.edu/class/cs21si/setup.html). We have some new dependencies this time, so don't skip this step!\n",
    "\n",
    "Run any code below by highlighting it and hitting `Shift + Enter`. Import the libraries below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Dense, Dropout, Activation, BatchNormalization\n",
    "from keras import regularizers\n",
    "from keras import backend as K\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "np.random.seed(1337)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in Data\n",
    "\n",
    "The data above was computed from a digitized image of a fine needle aspirate (FNA) of a breast mass. They describe characteristics of the cell nuclei present in the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_dataset(path):\n",
    "    dataset = pd.read_csv(path)\n",
    "    np.random.seed(42)\n",
    "    dataset = dataset.reindex(np.random.permutation(dataset.index))\n",
    "    return dataset\n",
    "\n",
    "dataset = get_dataset('resources/cancer-dataset.csv')\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separate Data and Labels\n",
    "\n",
    "Now, we'll use functions to get the data and the labels (i.e. the $X$ and the $y$). We want to make sure these return NumPy arrays so they can be passed into Keras models. Benign samples will be assigned the label 0, and malignant samples will be assigned the label 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_data(dataset):\n",
    "    data = np.array(dataset.as_matrix()[:, 2:-1], dtype=np.float64) # remove first, second, and last column\n",
    "    return data\n",
    "\n",
    "def get_labels(dataset):\n",
    "    diagnoses = dataset['diagnosis'].map({'M':1, 'B':0})\n",
    "    return np.array(diagnoses.as_matrix(), dtype=np.uint8)\n",
    "\n",
    "data, labels = get_data(dataset), get_labels(dataset)\n",
    "\n",
    "print(\"Number of patient samples: \", data.shape[0])\n",
    "print(\"Number of features per patient: \", data.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partition into Train, Validation, and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_data(data, labels, split):\n",
    "    train_ratio, val_ratio, test_ratio = split\n",
    "    num_examples = labels.shape[0]\n",
    "    train_bound, val_bound = int(train_ratio*num_examples), int(train_ratio*num_examples) + int(val_ratio*num_examples)\n",
    "    \n",
    "    train = {'data': data[:train_bound], 'labels': labels[:train_bound]}\n",
    "    val = {'data': data[train_bound:val_bound], 'labels': labels[train_bound:val_bound]}\n",
    "    test = {'data': data[val_bound:], 'labels': labels[val_bound:]}\n",
    "    \n",
    "    return train, val, test\n",
    "    \n",
    "train, val, test = split_data(data, labels, (.7, .2, .1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Normalization\n",
    "\n",
    "We want to scale our data so that each feature has mean 0 and variance 1. This is useful because it improves the stability of training our neural network. This makes it possible to train using more sophisticated networks and get better results. This is related to batch normalizationâ€“during batch normalization we are performing a similar operation, just on the inputs to a layer rather than the inputs into a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(train['data'])\n",
    "train['data'] = scaler.transform(train['data'])\n",
    "val['data'] = scaler.transform(val['data'])\n",
    "test['data'] = scaler.transform(test['data'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jupyter Exercise 1: Create your Own Classifier\n",
    "\n",
    "**Your task:** And the training wheels are coming off! Here, we want a 4-layer fully-connected neural network that can be used for binary classification. When you are coming up with layer sizes, the intuition that each layer has about half the number of units as the previous one can be helpful. Good luck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "epochs = 25\n",
    "num_classes = 2\n",
    "num_features = data.shape[1]\n",
    "\n",
    "def nn_classifier(learning_rate=0.005):\n",
    "    model = Sequential()\n",
    "    \n",
    "    # YOUR CODE HERE:\n",
    "\n",
    "    # END CODE\n",
    "    \n",
    "    # compile model\n",
    "    sgd = keras.optimizers.SGD(lr = learning_rate)\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "model = nn_classifier()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a summary of the structure of our model. Now let's train and evaluate our model! We want to not only train the model with many epochs, but also print the validation set accuracy at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def eval(model, train, val, num_epochs):\n",
    "    # fit the model\n",
    "    model.fit(train['data'], train['labels'], \n",
    "              epochs = num_epochs, \n",
    "              batch_size = 16, \n",
    "              verbose = 2,\n",
    "              shuffle = False)\n",
    "    \n",
    "    # evaluate the model\n",
    "    scores = model.evaluate(val['data'], val['labels'], batch_size=16, verbose=0)\n",
    "    \n",
    "    return scores\n",
    "\n",
    "loss, accuracy = eval(model, train, val, epochs)\n",
    "print(\"Validation Set Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jupyter Exercise 2: Hyperparameter Tuning\n",
    "\n",
    "**Your task**: Tuning the hyperparameters and developing intuition for how they affect the final performance is a large part of using neural networks, so we want you to get a lot of practice. Below, you should experiment with different values of the various hyperparameters, including learning rate, regularization strength, and dropout strength. Your goal in this exercise is to get as good of a result on the breast cancer dataset as you can, with a fully-connected deep neural network. Feel free to change the model you initialized above in Exercise 1 as well. There is no starter code here, so feel free to perform a hyperparameter sweep as you wish. You will find the code for *tune_hyperparams* from the Week 5 homework useful as a start, but note that the *nn_classifier* as it is right now only takes in an optional learning rate and not the dropout and regularization hyperparameters from before, so you will have to manually add most other hyperparameters if you want to play with these. Also note that the *eval* function needs be passed *model*, *train*, *val*, and *epochs*. **Aim for at least 97% accuracy on the validation set with your best model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tune_hyperparams():\n",
    "    best_model = (None, None, None)\n",
    "    ### YOUR CODE HERE\n",
    "\n",
    "                \n",
    "    ### END CODE\n",
    "            \n",
    "    return best_model\n",
    "        \n",
    "best_model = tune_hyperparams()\n",
    "print(\"\\n\\nBest Model Performance on Validation: \", best_model[1])\n",
    "print(\"Hyperparameters of Best Model: \", best_model[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Model on Unseen Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Try playing around with hyperparameters like the learning rate, size of the hidden layers, number of epochs, etc. until you get a model that you are satisfied with! Use validation accuracy to compare performance across different model configurations. Once you're done configuring, try testing on a completely unseen dataset to get a good idea of how your model will perform for unseen data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_score = best_model[0].evaluate(test['data'], test['labels'], batch_size=128)\n",
    "print(\"Test accuracy: %.2f\" % (test_score[1]))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
