{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 5 Homework - Solutions\n",
    "\n",
    "This week's homework is focused on making further improvements to our COMPAS-like deep neural network. We will be performing hyperparameter tuning. As discussed during lecture, you might not always see huge improvements from regularization or dropout. Oftentimes, this is because **hyperparameters** including regularization and dropout strengths are not optimal. Hyperparameter tuning trains several different models, each with a different combinations of hyperparameter values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the Environment\n",
    "\n",
    "You should have already cloned the `unit3` repository from GitHub and installed dependencies in requirements.txt [(instructions here, just in case)](http://web.stanford.edu/class/cs21si/setup.html) during class.\n",
    "\n",
    "Run any code below by highlighting it and hitting `Shift + Enter`. Import the libraries below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, BatchNormalization\n",
    "from keras import regularizers\n",
    "from keras import backend as K\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "np.random.seed(1337)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read and Clean in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filename = \"resources/compas-scores.csv\"\n",
    "\n",
    "# load data\n",
    "data = pd.read_csv(filename, header = 0)\n",
    "\n",
    "# select fields we want\n",
    "fields_of_interest = ['name', 'sex', 'age', 'race', 'priors_count', 'c_charge_desc', \n",
    "                      'v_decile_score', 'decile_score', 'is_violent_recid', 'is_recid']\n",
    "data = data[fields_of_interest]\n",
    "data.columns = ['name', 'sex', 'age', 'race', 'num_priors', 'charge', \n",
    "                'violence_score', 'recidivism_score', 'violence_true', 'recidivism_true']\n",
    "\n",
    "# remove records with missing scores\n",
    "data = data.loc[(data.violence_score != -1) & (data.recidivism_score != -1)]\n",
    "data = data.loc[(data.violence_true != -1) & (data.recidivism_true != -1)]\n",
    "\n",
    "# convert strings to numerical values\n",
    "races = ['African-American', 'Asian', 'Caucasian', 'Hispanic', 'Native American', 'Other']\n",
    "sex_classes = {'Male': 0, 'Female' : 1}\n",
    "race_classes = {races[i]: i for i in range(len(races))}\n",
    "\n",
    "# 'Other': 0, 'Caucasian': 1, 'African-American': 2, 'Hispanic': 3, 'Asian': 4, 'Native American': 5}\n",
    "data['sex'] = data['sex'].apply(lambda x: sex_classes[x])\n",
    "data['race'] = data['race'].apply(lambda x: race_classes[x])\n",
    "# threshold and binarize scores\n",
    "data['violence_score'] = data['violence_score'].apply(lambda x: 0 if x <= 5 else 1)\n",
    "data['recidivism_score'] = data['recidivism_score'].apply(lambda x: 0 if x <= 5 else 1)\n",
    "print(data)\n",
    "\n",
    "# convert pandas dataframe to numpy array for easier processing\n",
    "data = data.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partition into Train and Test Sets\n",
    "\n",
    "This was all code you wrote during lecture, so we've given it to you as a freebie here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# split into input (X) and output (Y) variables\n",
    "X = data[:,1:5] # sex, age, race, num_priors\n",
    "y = data[:,7] # recidivism_score\n",
    "\n",
    "num_train = int(math.ceil(X.shape[0]*0.8))\n",
    "num_test = int(math.floor(X.shape[0]*0.2))\n",
    "\n",
    "#########################################################\n",
    "# Returns the specified records of a given array, from\n",
    "# row_start to row_start + num_rows - 1 (inclusive).\n",
    "#########################################################\n",
    "def get_rows(dataset, row_start, num_rows):\n",
    "    return dataset[row_start:row_start + num_rows]\n",
    "\n",
    "X_train = get_rows(X, 0, num_train)\n",
    "y_train = get_rows(y, 0, num_train)\n",
    "\n",
    "X_test = get_rows(X, num_train, num_test)\n",
    "y_test = get_rows(y, num_train, num_test)\n",
    "\n",
    "num_classes = 2\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "print(X_train.shape[0], 'records in train set')\n",
    "print(X_test.shape[0], 'records in test set')\n",
    "print(X.shape[0], 'records in total')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up our Evaluation Pipeline\n",
    "\n",
    "Again, we wrote this together in class, so it's given to you here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#########################################################\n",
    "# Trains and evaluates given model. Returns loss and \n",
    "# accuracy.\n",
    "#########################################################\n",
    "def eval(model, verb = 2):\n",
    "    # fit the model\n",
    "    model.fit(X_train, y_train, \n",
    "              epochs = epochs, \n",
    "              batch_size = batch_size,          \n",
    "              validation_split = 0.1,\n",
    "              verbose = verb,\n",
    "              shuffle = False)\n",
    "    \n",
    "    # evaluate the model\n",
    "    scores = model.evaluate(X_test, y_test)\n",
    "    \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jupyter Exercise 1: Create and Evaluate your Own Classifier\n",
    "\n",
    "During lecture, you learned about adding `Dropout`, `Dense` layers (which are just fully-connected layers that have the parameter `kernel_regularizer`), and `BatchNormalization` to a Keras model. Check out this [guide](https://keras.io/getting-started/sequential-model-guide/) for some good examples in case you want a refresher.\n",
    "\n",
    "**Your task:** Now that you've gotten your feet wet with Keras, in this exercise, you get to decide how you want to set up your model! Add layers of any size (paired with any activation functions) as you see fit, and if you want to experiment with other Keras add-ons that we haven't discussed before, then go for it. Similarly, if you elect not to use L2-regularization or dropout, then that's up to you too. Good luck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "epochs = 15\n",
    "num_classes = 2\n",
    "\n",
    "learning_rate = 5e-2\n",
    "reg_strength = 0.15\n",
    "dropout_strength = 0.1\n",
    "\n",
    "#########################################################\n",
    "# Initializes neural network with dropout.\n",
    "#########################################################\n",
    "def nn_classifier(learning_rate, reg_strength, dropout_strength):\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "\n",
    "    # YOUR CODE HERE:\n",
    "    model.add(Dense(50, input_dim = X.shape[1], activation = 'relu')) \n",
    "    model.add(Dense(100, activation = 'relu', kernel_regularizer=regularizers.l2(reg_strength))) \n",
    "    model.add(Dense(50, activation = 'relu', kernel_regularizer=regularizers.l2(reg_strength))) \n",
    "    model.add(Dense(num_classes, activation = 'softmax'))\n",
    "    # END CODE\n",
    "    \n",
    "    # compile model\n",
    "    sgd = keras.optimizers.SGD(lr = learning_rate)\n",
    "    model.compile(loss = keras.losses.categorical_crossentropy, \n",
    "                  optimizer = sgd, metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Evaluate your model\n",
    "model = nn_classifier(learning_rate, reg_strength, dropout_strength)\n",
    "loss, acc = eval(model, verb = 0)\n",
    "print('\\n\\nTest loss:', loss)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jupyter Exercise 2: Hyperparameter Tuning\n",
    "\n",
    "For the purposes of playing around with hyperparameter tuning and understanding the motivations behind it, we're going to be working with a naive implementation below.  \n",
    "\n",
    "**Your task**: Tuning the hyperparameters and developing intuition for how they affect the final performance is a large part of using neural networks, so we want you to get a lot of practice. Below, you should experiment with different values of the various hyperparameters, including learning rate, regularization strength, and dropout strength. Your goal in this exercise is to get as good of a result on the COMPAS dataset as you can, with a fully-connected deep neural network. Feel free to change the model you initialized above in Exercise 1 as well. The starter code below is there to give you a naive implementation of hyperparameter tuning. You can change it as you wish. **Aim for at least 75% accuracy on the test set with your best model.** Good luck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tune_hyperparams():\n",
    "    best_model = (None, None, None)\n",
    "    running_best_accuracy = 0\n",
    "\n",
    "    learning_rate = [1e-12, 1e-10, 5e-8]\n",
    "    reg_strength = [1e-2] \n",
    "    dropout_strength = [0.1]\n",
    "    \n",
    "    for lr in learning_rate:\n",
    "        for reg in reg_strength:\n",
    "            for drop in dropout_strength:\n",
    "                model = nn_classifier(lr, reg, drop)\n",
    "                model_loss, model_acc = eval(model)\n",
    "        \n",
    "                print('\\n val_acc: {:f}, lr: {:f}, reg: {:f}, drop: {:f}\\n'.format(\n",
    "                        model_acc, lr, reg, drop))\n",
    "        \n",
    "                if model_acc > running_best_accuracy:\n",
    "                    model_params = {\"lr\": lr, \"reg\": reg, \"drop\": drop}\n",
    "                    best_model = (model, model_acc, model_params)\n",
    "                    running_best_accuracy = model_acc\n",
    "            \n",
    "    return best_model\n",
    "        \n",
    "best_model = tune_hyperparams()\n",
    "print(\"\\n\\nBest Model Performance: \", best_model[1])\n",
    "print(\"Hyperparameters of Best Model: \", best_model[2])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
